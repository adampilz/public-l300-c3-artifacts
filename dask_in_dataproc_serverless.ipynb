{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312f7465-1b19-4955-8ea3-adcf71935ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a56360a-8cc2-49b9-8742-6768e9cc547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "from datetime import datetime\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, Metrics, Model, Output, component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e85ce3-4850-4cde-8c0e-e992be9eea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project variables\n",
    "PROJECT_ID = \"your-project\"\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = f\"bkt-{PROJECT_ID}\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d57d9-1f4c-4ae9-a5ca-bec5b36a493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# spark program\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc810a-ec99-4eed-96e7-76470a8944eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wordcount.py\n",
    "\n",
    "\"\"\"A PySpark program that counts the number of words in Shakespeare.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def run(argv=None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input',\n",
    "                        dest='input',\n",
    "                        default=' ',\n",
    "                        help='Input file to process.')\n",
    "    parser.add_argument('--output',\n",
    "                        dest='output',\n",
    "                        default='gs://YOUR_OUTPUT_BUCKET/AND_OUTPUT_PREFIX',\n",
    "                        help='Output file to write results to.')\n",
    "    \n",
    "    known_args, _ = parser.parse_known_args(argv)\n",
    "    \n",
    "    spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"wordcount\")\\\n",
    "            .getOrCreate()\n",
    "    \n",
    "    sc = spark.sparkContext    \n",
    "    words = sc.textFile(known_args.input).flatMap(lambda line: line.split(\" \"))\n",
    "    wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)\n",
    "    wordCounts.saveAsTextFile(known_args.output)\n",
    "    \n",
    "    spark.stop()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    run(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f06b5a-c65a-4597-880b-162c284dc2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# dask program\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858590d8-11d1-4b67-9d43-033597a76767",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile daskcompute.py\n",
    "\n",
    "\"\"\"A dask program that runs on dataproc serverless.\"\"\"\n",
    "\n",
    "# dask\n",
    "import dask.array as da\n",
    "import time\n",
    "\n",
    "def run(argv=None):\n",
    "    start_time = time.time()\n",
    "    x = da.random.random((1000000, 1000000), chunks=(1000, 1000))\n",
    "    s = x.sum().compute()\n",
    "    print(f\"ADAM: {str(s)}\")\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time-start_time\n",
    "    print(f\"ADAM: {str(elapsed_time)}\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa63b4d-483d-4245-ac2f-6912a36ebb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy python module to Cloud Storage\n",
    "GCS_WC_PY = BUCKET_URI + \"/daskcompute.py\"\n",
    "! gsutil cp daskcompute.py $GCS_WC_PY\n",
    "\n",
    "# these lines are not necessary for this to run\n",
    "GCS_WC_OUT = BUCKET_URI + \"/wc_out/\"\n",
    "GCS_WC_IN = \"gs://dataproc-datasets-us-central1/shakespeare/all-lines.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4df8fa-698b-4574-b48c-ae8849aa756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/dataproc_pyspark\".format(BUCKET_URI)\n",
    "BATCH_ID = \"ap-dask-dataproc-serverless-\" + TIMESTAMP\n",
    "ARGS = [\n",
    "    \"--input\",\n",
    "    GCS_WC_IN,\n",
    "    \"--output\",\n",
    "    GCS_WC_OUT,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad491e-fd67-40b4-9302-e48cd5d373e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline def\n",
    "@dsl.pipeline(\n",
    "    name=\"dataproc-pyspark-with-dask\",\n",
    "    description=\"An exmaple pipeline that uses DataprocPySparkBatchOp for running a PySpark batch workload.\",\n",
    ")\n",
    "def pipeline(\n",
    "    batch_id: str = BATCH_ID,\n",
    "    project_id: str = PROJECT_ID,\n",
    "    location: str = REGION,\n",
    "    main_python_file_uri: str = GCS_WC_PY,\n",
    "    #service_account: str = SERVICE_ACCOUNT,\n",
    "    args: list = ARGS,\n",
    "):\n",
    "    from google_cloud_pipeline_components.experimental.dataproc import \\\n",
    "        DataprocPySparkBatchOp\n",
    "\n",
    "    _ = DataprocPySparkBatchOp(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        batch_id=batch_id,\n",
    "        main_python_file_uri=main_python_file_uri,\n",
    "        #service_account=service_account,\n",
    "        args=args,\n",
    "    )\n",
    "\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"pipeline\",\n",
    "    template_path=\"pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a07ccc-040d-4818-8095-0261d0170abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af5bcf4-950f-403a-9f50-b0e2f8a9049c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
